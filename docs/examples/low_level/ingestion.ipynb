{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/graja007/askui-try-out/blob/main/docs/examples/low_level/ingestion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57c676db",
      "metadata": {
        "id": "57c676db"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jerryjliu/llama_index/blob/main/docs/examples/low_level/ingestion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c919f307-07b1-41bd-bc5d-51edd8677983",
      "metadata": {
        "id": "c919f307-07b1-41bd-bc5d-51edd8677983"
      },
      "source": [
        "# Building Data Ingestion from Scratch\n",
        "\n",
        "In this tutorial, we show you how to build a data ingestion pipeline into a vector database.\n",
        "\n",
        "We use Pinecone as the vector database.\n",
        "\n",
        "We will show how to do the following:\n",
        "1. How to load in documents.\n",
        "2. How to use a text splitter to split documents.\n",
        "3. How to **manually** construct nodes from each text chunk.\n",
        "4. [Optional] Add metadata to each Node.\n",
        "5. How to generate embeddings for each text chunk.\n",
        "6. How to insert into a vector database."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tsHaUeqRpflK",
      "metadata": {
        "id": "tsHaUeqRpflK"
      },
      "source": [
        "# Pinecone\n",
        "\n",
        "You will need a [pinecone.io](https://www.pinecone.io/) api key for this tutorial. You can [sign up for free](https://app.pinecone.io/?sessionType=signup) to get a Starter account.\n",
        "\n",
        "If you create a Starter account, you can name your application anything you like.\n",
        "\n",
        "Once you have an account, navigate to 'API Keys' in the Pinecone console. You can use the default key or create a new one for this tutorial.\n",
        "\n",
        "Save your api key and its environment (`gcp_starter` for free accounts). You will need them below."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92b20306",
      "metadata": {
        "id": "92b20306"
      },
      "source": [
        "If you're opening this Notebook on colab, you will probably need to install LlamaIndex 🦙."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b60e707a",
      "metadata": {
        "id": "b60e707a",
        "outputId": "152a319f-c32c-49bd-8937-8697b94ac575",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting llama-index\n",
            "  Downloading llama_index-0.8.54-py3-none-any.whl (795 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m795.5/795.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: SQLAlchemy[asyncio]>=1.4.49 in /usr/local/lib/python3.10/dist-packages (from llama-index) (2.0.22)\n",
            "Collecting aiostream<0.6.0,>=0.5.2 (from llama-index)\n",
            "  Downloading aiostream-0.5.2-py3-none-any.whl (39 kB)\n",
            "Collecting dataclasses-json<0.6.0,>=0.5.7 (from llama-index)\n",
            "  Downloading dataclasses_json-0.5.14-py3-none-any.whl (26 kB)\n",
            "Collecting deprecated>=1.2.9.3 (from llama-index)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (2023.6.0)\n",
            "Collecting langchain>=0.0.303 (from llama-index)\n",
            "  Downloading langchain-0.0.325-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.5.8)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index) (3.8.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.23.5)\n",
            "Collecting openai>=0.26.4 (from llama-index)\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index) (1.5.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.2.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (8.2.3)\n",
            "Collecting tiktoken>=0.3.3 (from llama-index)\n",
            "  Downloading tiktoken-0.5.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index) (4.5.0)\n",
            "Collecting typing-inspect>=0.8.0 (from llama-index)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting urllib3<2 (from llama-index)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.6.0,>=0.5.7->llama-index)\n",
            "  Downloading marshmallow-3.20.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama-index) (1.14.1)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (6.0.1)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (3.8.6)\n",
            "Requirement already satisfied: anyio<4.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (3.7.1)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (4.0.3)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain>=0.0.303->llama-index)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.1.0,>=0.0.52 (from langchain>=0.0.303->llama-index)\n",
            "  Downloading langsmith-0.0.53-py3-none-any.whl (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.3/43.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (1.10.13)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.303->llama-index) (2.31.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index) (4.66.1)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect>=0.8.0->llama-index)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index) (2023.3.post1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (3.3.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.303->llama-index) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain>=0.0.303->llama-index) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain>=0.0.303->llama-index) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4.0->langchain>=0.0.303->llama-index) (1.1.3)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain>=0.0.303->llama-index)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->llama-index) (23.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain>=0.0.303->llama-index) (2023.7.22)\n",
            "Installing collected packages: urllib3, mypy-extensions, marshmallow, jsonpointer, deprecated, aiostream, typing-inspect, jsonpatch, tiktoken, openai, langsmith, dataclasses-json, langchain, llama-index\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aiostream-0.5.2 dataclasses-json-0.5.14 deprecated-1.2.14 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.325 langsmith-0.0.53 llama-index-0.8.54 marshmallow-3.20.1 mypy-extensions-1.0.0 openai-0.28.1 tiktoken-0.5.1 typing-inspect-0.9.0 urllib3-1.26.18\n"
          ]
        }
      ],
      "source": [
        "!pip install llama-index"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22fb9e0a-566b-4f34-b9cf-72193cb51adb",
      "metadata": {
        "id": "22fb9e0a-566b-4f34-b9cf-72193cb51adb"
      },
      "source": [
        "# OpenAI\n",
        "\n",
        "You will need an [OpenAI](https://openai.com/) api key for this tutorial. Login to your [platform.openai.com](https://platform.openai.com/) account, click on your profile picture in the upper right corner, and choose 'API Keys' from the menu. Create an API key for this tutorial and save it. You will need it below."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HPwWNeZwgwE8",
      "metadata": {
        "id": "HPwWNeZwgwE8"
      },
      "source": [
        "## Environment\n",
        "\n",
        "First we add our dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "CyTVgLfMgmIZ",
      "metadata": {
        "id": "CyTVgLfMgmIZ",
        "outputId": "c26a79ed-7373-46ce-adaa-1cbcde6214dc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.4/179.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.6/30.6 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip -q install python-dotenv pinecone-client llama-index pymupdf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bCwZFn6_iAR1",
      "metadata": {
        "id": "bCwZFn6_iAR1"
      },
      "source": [
        "#### Set Environment Variables\n",
        "\n",
        "We create a file for our environment variables. Do not commit this file or share it!\n",
        "\n",
        "Note: Google Colabs will let you create but not open a .env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M1l2emfWgjgE",
      "metadata": {
        "id": "M1l2emfWgjgE"
      },
      "outputs": [],
      "source": [
        "dotenv_path = (  # Google Colabs will not let you open a .env, but you can set\n",
        "    \"env\"\n",
        ")\n",
        "with open(dotenv_path, \"w\") as f:\n",
        "    f.write('PINECONE_API_KEY=\"<your api key>\"\\n')\n",
        "    f.write('PINECONE_ENVIRONMENT=\"gcp-starter\"\\n')\n",
        "    f.write('OPENAI_API_KEY=\"<your api key>\"\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PWMbn7GooMm5",
      "metadata": {
        "id": "PWMbn7GooMm5"
      },
      "source": [
        "Set your OpenAI api key, and Pinecone api key and environment in the file we created."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QOyfIoXAoVGX",
      "metadata": {
        "id": "QOyfIoXAoVGX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "leZkMBXYiTl-",
      "metadata": {
        "id": "leZkMBXYiTl-"
      },
      "outputs": [],
      "source": [
        "load_dotenv(dotenv_path=dotenv_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcb486eb-c0b8-40e2-9038-da97aef63139",
      "metadata": {
        "id": "bcb486eb-c0b8-40e2-9038-da97aef63139"
      },
      "source": [
        "## Setup\n",
        "\n",
        "We build an empty Pinecone Index, and define the necessary LlamaIndex wrappers/abstractions so that we can start loading data into Pinecone.\n",
        "\n",
        "\n",
        "Note: Do not save your API keys in the code or add pinecone_env to your repo!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0Izxlt0XkMII",
      "metadata": {
        "id": "0Izxlt0XkMII"
      },
      "outputs": [],
      "source": [
        "import pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc739b4d-491f-406d-a0e6-f6b1e8c126dc",
      "metadata": {
        "id": "cc739b4d-491f-406d-a0e6-f6b1e8c126dc"
      },
      "outputs": [],
      "source": [
        "api_key = os.environ[\"PINECONE_API_KEY\"]\n",
        "environment = os.environ[\"PINECONE_ENVIRONMENT\"]\n",
        "pinecone.init(api_key=api_key, environment=environment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Whwu7HqqswIq",
      "metadata": {
        "id": "Whwu7HqqswIq"
      },
      "outputs": [],
      "source": [
        "index_name = \"llamaindex-rag-fs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yRKkO4g1sBMl",
      "metadata": {
        "id": "yRKkO4g1sBMl"
      },
      "outputs": [],
      "source": [
        "# [Optional] Delete the index before re-running the tutorial.\n",
        "# pinecone.delete_index(index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20ba2f76-29d8-4dc5-b25c-64dcfe9e8d23",
      "metadata": {
        "id": "20ba2f76-29d8-4dc5-b25c-64dcfe9e8d23"
      },
      "outputs": [],
      "source": [
        "# dimensions are for text-embedding-ada-002\n",
        "pinecone.create_index(\n",
        "    index_name, dimension=1536, metric=\"euclidean\", pod_type=\"p1\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45f9a999-dac2-4bc8-8133-ccc851b76a6d",
      "metadata": {
        "id": "45f9a999-dac2-4bc8-8133-ccc851b76a6d"
      },
      "outputs": [],
      "source": [
        "pinecone_index = pinecone.Index(index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3216f9e2-946d-4b43-8b8c-acf6788633a5",
      "metadata": {
        "id": "3216f9e2-946d-4b43-8b8c-acf6788633a5"
      },
      "outputs": [],
      "source": [
        "# [Optional] drop contents in index - will not work on free accounts\n",
        "pinecone_index.delete(deleteAll=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89246384-983c-4e2c-ac05-ffdc1d54a594",
      "metadata": {
        "id": "89246384-983c-4e2c-ac05-ffdc1d54a594"
      },
      "source": [
        "#### Create PineconeVectorStore\n",
        "\n",
        "Simple wrapper abstraction to use in LlamaIndex. Wrap in StorageContext so we can easily load in Nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "775aabb2-3dd2-44b1-b6b9-2f7326409e10",
      "metadata": {
        "id": "775aabb2-3dd2-44b1-b6b9-2f7326409e10"
      },
      "outputs": [],
      "source": [
        "from llama_index.vector_stores import PineconeVectorStore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15f0aa46-9f5b-42c1-9374-db94781363f0",
      "metadata": {
        "id": "15f0aa46-9f5b-42c1-9374-db94781363f0"
      },
      "outputs": [],
      "source": [
        "vector_store = PineconeVectorStore(pinecone_index=pinecone_index)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be9437a0-3d52-4586-8217-43944971a2cc",
      "metadata": {
        "id": "be9437a0-3d52-4586-8217-43944971a2cc"
      },
      "source": [
        "## Build an Ingestion Pipeline from Scratch\n",
        "\n",
        "We show how to build an ingestion pipeline as mentioned in the introduction.\n",
        "\n",
        "Note that steps (2) and (3) can be handled via our `NodeParser` abstractions, which handle splitting and node creation.\n",
        "\n",
        "For the purposes of this tutorial, we show you how to create these objects manually."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d1c9630-2a6b-4656-b272-de1b869c8977",
      "metadata": {
        "id": "6d1c9630-2a6b-4656-b272-de1b869c8977"
      },
      "source": [
        "### 1. Load Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48739cfc-c05a-420a-8c78-280892f8d7a0",
      "metadata": {
        "id": "48739cfc-c05a-420a-8c78-280892f8d7a0",
        "outputId": "82b4bf5f-a2ca-4061-bee7-419a05c373bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-10-13 01:45:14--  https://arxiv.org/pdf/2307.09288.pdf\n",
            "Resolving arxiv.org (arxiv.org)... 128.84.21.199\n",
            "Connecting to arxiv.org (arxiv.org)|128.84.21.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13661300 (13M) [application/pdf]\n",
            "Saving to: ‘data/llama2.pdf’\n",
            "\n",
            "data/llama2.pdf     100%[===================>]  13.03M  7.59MB/s    in 1.7s    \n",
            "\n",
            "2023-10-13 01:45:16 (7.59 MB/s) - ‘data/llama2.pdf’ saved [13661300/13661300]\n"
          ]
        }
      ],
      "source": [
        "!mkdir data\n",
        "!wget --user-agent \"Mozilla\" \"https://arxiv.org/pdf/2307.09288.pdf\" -O \"data/llama2.pdf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "079666c5-0685-413d-a765-17f71ae89506",
      "metadata": {
        "id": "079666c5-0685-413d-a765-17f71ae89506"
      },
      "outputs": [],
      "source": [
        "import fitz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4eee7692-2188-4552-9f2e-cb90ac6b7678",
      "metadata": {
        "id": "4eee7692-2188-4552-9f2e-cb90ac6b7678"
      },
      "outputs": [],
      "source": [
        "file_path = \"./data/llama2.pdf\"\n",
        "doc = fitz.open(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74c573db-1863-45c3-9049-8bad535e6e35",
      "metadata": {
        "id": "74c573db-1863-45c3-9049-8bad535e6e35"
      },
      "source": [
        "### 2. Use a Text Splitter to Split Documents\n",
        "\n",
        "Here we import our `SentenceSplitter` to split document texts into smaller chunks, while preserving paragraphs/sentences as much as possible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e175007-84d5-406e-bf5f-6ecacfbfd152",
      "metadata": {
        "id": "9e175007-84d5-406e-bf5f-6ecacfbfd152"
      },
      "outputs": [],
      "source": [
        "from llama_index.text_splitter import SentenceSplitter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0dbccb26-ea2a-48c9-adb4-1ebe88adaa1c",
      "metadata": {
        "id": "0dbccb26-ea2a-48c9-adb4-1ebe88adaa1c"
      },
      "outputs": [],
      "source": [
        "text_splitter = SentenceSplitter(\n",
        "    chunk_size=1024,\n",
        "    # separator=\" \",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a9bed96-adfa-40c9-92bd-9dba68d58730",
      "metadata": {
        "id": "7a9bed96-adfa-40c9-92bd-9dba68d58730"
      },
      "outputs": [],
      "source": [
        "text_chunks = []\n",
        "# maintain relationship with source doc index, to help inject doc metadata in (3)\n",
        "doc_idxs = []\n",
        "for doc_idx, page in enumerate(doc):\n",
        "    page_text = page.get_text(\"text\")\n",
        "    cur_text_chunks = text_splitter.split_text(page_text)\n",
        "    text_chunks.extend(cur_text_chunks)\n",
        "    doc_idxs.extend([doc_idx] * len(cur_text_chunks))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "354157d6-b436-4f0a-bf6e-f0a197e54c60",
      "metadata": {
        "id": "354157d6-b436-4f0a-bf6e-f0a197e54c60"
      },
      "source": [
        "### 3. Manually Construct Nodes from Text Chunks\n",
        "\n",
        "We convert each chunk into a `TextNode` object, a low-level data abstraction in LlamaIndex that stores content but also allows defining metadata + relationships with other Nodes.\n",
        "\n",
        "We inject metadata from the document into each node.\n",
        "\n",
        "This essentially replicates logic in our `SimpleNodeParser`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33b93044-3eb4-4c77-bc40-be53dffd3749",
      "metadata": {
        "id": "33b93044-3eb4-4c77-bc40-be53dffd3749"
      },
      "outputs": [],
      "source": [
        "from llama_index.schema import TextNode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adbfcb3f-5554-4594-ae80-7236e28485aa",
      "metadata": {
        "id": "adbfcb3f-5554-4594-ae80-7236e28485aa"
      },
      "outputs": [],
      "source": [
        "nodes = []\n",
        "for idx, text_chunk in enumerate(text_chunks):\n",
        "    node = TextNode(\n",
        "        text=text_chunk,\n",
        "    )\n",
        "    src_doc_idx = doc_idxs[idx]\n",
        "    src_page = doc[src_doc_idx]\n",
        "    nodes.append(node)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3iidPVIiwYUg",
      "metadata": {
        "id": "3iidPVIiwYUg"
      },
      "outputs": [],
      "source": [
        "print(nodes[0].metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "257bb2e3-608a-4542-ba29-f29b59771a3f",
      "metadata": {
        "id": "257bb2e3-608a-4542-ba29-f29b59771a3f"
      },
      "outputs": [],
      "source": [
        "# print a sample node\n",
        "print(nodes[0].get_content(metadata_mode=\"all\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fac468f5-870c-4486-b576-2aa6d9eaf322",
      "metadata": {
        "id": "fac468f5-870c-4486-b576-2aa6d9eaf322"
      },
      "source": [
        "### [Optional] 4. Extract Metadata from each Node\n",
        "\n",
        "We extract metadata from each Node using our Metadata extractors.\n",
        "\n",
        "This will add more metadata to each Node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c369188-9cc9-4550-924e-b29d212ad057",
      "metadata": {
        "id": "9c369188-9cc9-4550-924e-b29d212ad057"
      },
      "outputs": [],
      "source": [
        "from llama_index.node_parser.extractors import (\n",
        "    MetadataExtractor,\n",
        "    QuestionsAnsweredExtractor,\n",
        "    TitleExtractor,\n",
        ")\n",
        "from llama_index.llms import OpenAI\n",
        "\n",
        "llm = OpenAI(model=\"gpt-3.5-turbo\")\n",
        "\n",
        "metadata_extractor = MetadataExtractor(\n",
        "    extractors=[\n",
        "        TitleExtractor(nodes=5, llm=llm),\n",
        "        QuestionsAnsweredExtractor(questions=3, llm=llm),\n",
        "    ],\n",
        "    in_place=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5501ffc-9bbb-4b48-9181-4e4e371e8f41",
      "metadata": {
        "id": "f5501ffc-9bbb-4b48-9181-4e4e371e8f41"
      },
      "outputs": [],
      "source": [
        "nodes = metadata_extractor.process_nodes(nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WgbKmXr3ytPf",
      "metadata": {
        "id": "WgbKmXr3ytPf"
      },
      "outputs": [],
      "source": [
        "print(nodes[0].metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d52522c-ffee-49d1-8651-658d70248053",
      "metadata": {
        "id": "9d52522c-ffee-49d1-8651-658d70248053"
      },
      "source": [
        "### 5. Generate Embeddings for each Node\n",
        "\n",
        "Generate document embeddings for each Node using our OpenAI embedding model (`text-embedding-ada-002`).\n",
        "\n",
        "Store these on the `embedding` property on each Node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e071e36-e609-4a0c-a478-e8cfbe751cff",
      "metadata": {
        "id": "6e071e36-e609-4a0c-a478-e8cfbe751cff"
      },
      "outputs": [],
      "source": [
        "from llama_index.embeddings import OpenAIEmbedding\n",
        "\n",
        "embed_model = OpenAIEmbedding()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2047ca46-729f-4c5a-a8d7-3bc860604333",
      "metadata": {
        "id": "2047ca46-729f-4c5a-a8d7-3bc860604333"
      },
      "outputs": [],
      "source": [
        "for node in nodes:\n",
        "    node_embedding = embed_model.get_text_embedding(\n",
        "        node.get_content(metadata_mode=\"all\")\n",
        "    )\n",
        "    node.embedding = node_embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11f78014-dcca-43f5-95cb-9cfb5140b30e",
      "metadata": {
        "id": "11f78014-dcca-43f5-95cb-9cfb5140b30e"
      },
      "source": [
        "### 6. Load Nodes into a Vector Store\n",
        "\n",
        "We now insert these nodes into our `PineconeVectorStore`.\n",
        "\n",
        "**NOTE**: We skip the VectorStoreIndex abstraction, which is a higher-level abstraction that handles ingestion as well. We use `VectorStoreIndex` in the next section to fast-track retrieval/querying."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe34fbe4-3396-402e-8599-4b42013c3016",
      "metadata": {
        "id": "fe34fbe4-3396-402e-8599-4b42013c3016"
      },
      "outputs": [],
      "source": [
        "vector_store.add(nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74e7f8bd-d92a-40ad-8d9b-a18c04ddfca9",
      "metadata": {
        "id": "74e7f8bd-d92a-40ad-8d9b-a18c04ddfca9"
      },
      "source": [
        "## Retrieve and Query from the Vector Store\n",
        "\n",
        "Now that our ingestion is complete, we can retrieve/query this vector store.\n",
        "\n",
        "**NOTE**: We can use our high-level `VectorStoreIndex` abstraction here. See the next section to see how to define retrieval at a lower-level!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be6a4fe1-2665-43e6-a872-8e631e31b0fd",
      "metadata": {
        "id": "be6a4fe1-2665-43e6-a872-8e631e31b0fd"
      },
      "outputs": [],
      "source": [
        "from llama_index import VectorStoreIndex\n",
        "from llama_index.storage import StorageContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e9d2495-d4f7-469a-9cea-a5cfc401c085",
      "metadata": {
        "id": "0e9d2495-d4f7-469a-9cea-a5cfc401c085"
      },
      "outputs": [],
      "source": [
        "index = VectorStoreIndex.from_vector_store(vector_store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c89e1c1-8ed1-45f5-b2a4-7c3382195693",
      "metadata": {
        "id": "5c89e1c1-8ed1-45f5-b2a4-7c3382195693"
      },
      "outputs": [],
      "source": [
        "query_engine = index.as_query_engine()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd6e0ddb-97c9-4f42-8843-a36a29ba3f17",
      "metadata": {
        "id": "cd6e0ddb-97c9-4f42-8843-a36a29ba3f17"
      },
      "outputs": [],
      "source": [
        "query_str = \"Can you tell me about the key concepts for safety finetuning\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "950cae37-7bad-44a3-be51-4154a8630818",
      "metadata": {
        "id": "950cae37-7bad-44a3-be51-4154a8630818"
      },
      "outputs": [],
      "source": [
        "response = query_engine.query(query_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0b309bb-ca5a-4b15-948c-687038361c91",
      "metadata": {
        "id": "c0b309bb-ca5a-4b15-948c-687038361c91"
      },
      "outputs": [],
      "source": [
        "print(str(response))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "llama_index_v2",
      "language": "python",
      "name": "llama_index_v2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}